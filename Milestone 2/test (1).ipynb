{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install imblearn\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Function to lemmatize tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = clean_text(text)  # Clean text\n",
        "    tokens = tokenize_text(text)  # Tokenize text\n",
        "    tokens = remove_stopwords(tokens)  # Remove stop words\n",
        "    tokens = lemmatize_tokens(tokens)  # Lemmatize tokens\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Provide the path to your Excel file\n",
        "excel_file_path = '/content/Dataset - Dataset.csv'\n",
        "\n",
        "# Load data from Excel\n",
        "try:\n",
        "    data = pd.read_csv(excel_file_path, encoding='latin1')\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {excel_file_path}\")\n",
        "    exit()\n",
        "\n",
        "# Drop rows with missing values in 'JD' column\n",
        "data = data.dropna(subset=['JD'])\n",
        "\n",
        "# Preprocess the text columns\n",
        "data['cleaned_resume'] = data['Resumes'].apply(preprocess_text)\n",
        "data['cleaned_jd'] = data['JD'].apply(preprocess_text)\n",
        "\n",
        "# Combine the cleaned resumes and job descriptions\n",
        "data['combined_text'] = data['cleaned_resume'] + ' ' + data['cleaned_jd']\n",
        "\n",
        "# Define the TF-IDF Vectorizer with n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
        "\n",
        "# Vectorize the combined text data\n",
        "X = vectorizer.fit_transform(data['combined_text'])\n",
        "y = data['Result']\n",
        "\n",
        "# Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform hyperparameter tuning using GridSearchCV for Random Forest\n",
        "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "# Train the best Random Forest model\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test data using Random Forest\n",
        "y_pred_proba_rf = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the Random Forest model (optional step)\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Use the best Random Forest model for prediction on the whole dataset\n",
        "data['prediction'] = best_rf_model.predict(vectorizer.transform(data['combined_text']))\n",
        "data['matching_score'] = best_rf_model.predict_proba(X)[:, 1]  # Predict matching score\n",
        "\n",
        "# Print data with predictions and matching scores\n",
        "print(data[['Resumes', 'JD', 'Result', 'prediction', 'matching_score']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOcb2LQOaZ2N",
        "outputId": "17980d37-71b4-4d2f-b012-28c55573a7ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "Random Forest model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.67      0.57         6\n",
            "           1       0.80      0.67      0.73        12\n",
            "\n",
            "    accuracy                           0.67        18\n",
            "   macro avg       0.65      0.67      0.65        18\n",
            "weighted avg       0.70      0.67      0.68        18\n",
            "\n",
            "                                              Resumes  \\\n",
            "0   JOHANN BACH FrontEnd Developer Portland OR Lin...   \n",
            "1   ALEKS LUDKEE FullStack Developer Nashville TN ...   \n",
            "2   Madalin Auton IOS Developer Louisville KY Educ...   \n",
            "3   YOUR NAME Phone Email Location City State ZIP ...   \n",
            "4   Karen Santos Senior FrontEnd Developer Brookly...   \n",
            "..                                                ...   \n",
            "81  PRAJWAL RAMANNA VENKATESH SoftwareEngineer Jav...   \n",
            "82  Mritunjay Pandey Thane Maharashtra India linke...   \n",
            "83  SURAJ PATRA CONTACT Work email ID Location New...   \n",
            "84  resume indhu ee email inashgmailcom mobile lin...   \n",
            "85  resume indhu ee email inashgmailcom mobile lin...   \n",
            "\n",
            "                                                   JD  Result  prediction  \\\n",
            "0   FrontEnd Developer Job Description We are look...       1           1   \n",
            "1   Full stack Developer job description Job descr...       1           1   \n",
            "2   Job Description SoftwareAI Engineer Overview A...       0           0   \n",
            "3   Full stack Developer job description Job descr...       0           0   \n",
            "4   FrontEnd Developer Job Description We are look...       1           1   \n",
            "..                                                ...     ...         ...   \n",
            "81  Department overview Sales specialist is a key ...       0           0   \n",
            "82  Responsibilities Plan and oversee all aspects ...       0           0   \n",
            "83  Responsibilities Implement machine learning al...       0           0   \n",
            "84  sample job description job title human resourc...       0           0   \n",
            "85  sample web developer job description internet ...       1           1   \n",
            "\n",
            "    matching_score  \n",
            "0         0.551810  \n",
            "1         0.691263  \n",
            "2         0.195472  \n",
            "3         0.157280  \n",
            "4         0.765679  \n",
            "..             ...  \n",
            "81        0.203258  \n",
            "82        0.211291  \n",
            "83        0.212857  \n",
            "84        0.224254  \n",
            "85        0.680019  \n",
            "\n",
            "[85 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to read and extract text from a PDF\n",
        "def read_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    document = fitz.open(file_path)\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        pdf_text += page.get_text()\n",
        "    return pdf_text\n",
        "\n",
        "# Function to preprocess a single JD and resume PDF and make prediction\n",
        "def test_single_jd_resume(resume_path, jd_path, model, vectorizer):\n",
        "    # Read and preprocess the resume and job description\n",
        "    resume_text = preprocess_text(read_pdf(resume_path))\n",
        "    jd_text = preprocess_text(read_pdf(jd_path))\n",
        "\n",
        "    # Combine the texts\n",
        "    combined_text = resume_text + ' ' + jd_text\n",
        "\n",
        "    # Vectorize the combined text\n",
        "    combined_vector = vectorizer.transform([combined_text])\n",
        "\n",
        "    # Predict using the trained model\n",
        "    prediction = model.predict(combined_vector)[0]\n",
        "\n",
        "    # Predict the matching score (probability of class 1)\n",
        "    matching_score = model.predict_proba(combined_vector)[0][1]\n",
        "\n",
        "    return prediction, matching_score\n",
        "\n",
        "# Provide paths to the resume and job description PDFs\n",
        "resume_path = '/content/Resume_Prasad.pdf'\n",
        "jd_path = '/content/Job-desc-sample (1).pdf'\n",
        "\n",
        "# Test single JD and resume\n",
        "prediction, matching_score = test_single_jd_resume(resume_path, jd_path, best_rf_model, vectorizer)\n",
        "\n",
        "# Print the prediction and matching score\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Matching Score: {matching_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEoV7UGraoAi",
        "outputId": "fe719414-f9a9-49b2-9072-4c5ad525b430"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 0\n",
            "Matching Score: 0.4380155873538224\n"
          ]
        }
      ]
    }
  ]
}